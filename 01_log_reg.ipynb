{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inspired by neural network & machine learning labs led by [GMUM](https://gmum.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "[PyTorch](https://pytorch.org/) is an open source machine learning framework, especially useful for deep learning (and what we'll be using for this course). The interface is quite similar to NumPy, but it additionally automatically calculates gradients and has GPU support (which is very useful for deep learning).\n",
    "\n",
    "If you want some additional sources, check out [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) and the [examples](https://github.com/pytorch/examples) from the PyTorch repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "The basic object on which we operate in PyTorch is `torch.Tensor`. They behave similarly to NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a = torch.Tensor([[1,2], [3,4]])  # create a tensor directly from numbers\n",
    "print(a)\n",
    "print(a.shape)  # shows us the shape of the tensor\n",
    "print(a.dtype)  # shows us the data type of the values of the tensor\n",
    "print(a.int().dtype)  # cast a tensor to a different type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_np = np.array([[1,2], [3,4]])\n",
    "b = torch.Tensor(b_np)  # create a tensor from a NumPy array\n",
    "print(b)\n",
    "print(b[:,1:])  # slicing and indexing works just like in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.ones((2,2))  # create a 2x2 tensor of ones \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.eye(3)  # create a 3x3 identity tensor\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.randn((2,1,2))  # create a tensor of random values from N(0,1) of shape (2,1,2)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = a * b  # element-wise multiplication\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.Tensor([1,2]) + torch.Tensor([1]))  # broadcasting works like in NumPy\n",
    "g = a + e  # a has shape (2,2), e has shape (2,1,2), so after broadcasting we get (2,2,2)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)\n",
    "h = c @ torch.tensor([[2.],[1.]])  # matrix multiplication \n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.log())  # computes the element-wise logarithm\n",
    "print(a.exp())  # computes the element-wise exponential function\n",
    "print(a.sum(dim=1))  # computes the sum along a given dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(a)\n",
    "print(a.view(1, 1, 4))  # reshapes the tensor to new dimensions(cf. np.reshape)\n",
    "print(a.view(-1))  # if you pass -1, the dimension will be inferred (e.g. easy way to flatten a tensor)\n",
    "print(torch.Tensor([2]).item()) # we can use .item() to get a number from a tensor with a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "For ML, we generally need three things:\n",
    "* a model,\n",
    "* a cost function,\n",
    "* an optimization method.\n",
    "\n",
    "Today, we'll be training *logistic regression* with the *cross entropy loss* using *stochastic gradient descent* in PyTorch.\n",
    "First, let's recap what we know:\n",
    "\n",
    "## Binomial logistic regression\n",
    "(Binomial) *[logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)* is supervised classification method, i.e. we are presented with a dataset $\\{(\\mathbf{x}^i, y^i) : i=1\\ldots N\\}$, where $\\mathbf{x}^i=[x^i_1\\ldots x^i_D]$ is a vector of features, $y^i \\in \\{0,1\\}$ is a discrete outcome label, $N$ is the number of examples, and we want to predict $y^i$ from $\\mathbf{x}^i$ (e.g. we want to predict whether someone has cancer based on several test results). Let $p$ denote the probability that a given example has $y=1$. \n",
    "\n",
    "We assume a linear relationship between the *logit* (or *log-odds*) $\\mathscr{l}$ and the predictor variables: $$ \\mathscr{l}=\\ln\\big(\\frac{p}{1-p}\\big) = \\theta_1 x_1 + \\ldots + \\theta_D x_D + b= \\theta^T \\mathbf{x} + b,$$ where $\\theta$ and $b$ are the parameters of the model ($b$ is often called the *bias*). Exponentiating both sides of the above equation, we get $$\\frac{p}{1-p} = e^{\\mathscr{l}}.$$ From this we can recover $p$ as $$p=\\frac{e^\\mathscr{l}}{1+e^\\mathscr{l}} = \\frac{1}{1+e^{-\\mathscr{l}}}.$$ $\\sigma(t)=\\frac{1}{1+e^{-t}}$ is the so-called *logistic function*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"$\\sigma$(t)\")\n",
    "_ = plt.plot(np.linspace(-6, 6), 1 / (1 + np.exp(-np.linspace(-6, 6))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our learning objective we will want to maximize the *log-likelihood* (or minimize the negative log-likelihood), which in this case is equivalent to minimizing the *cross entropy*: $$-\\frac{1}{N}\\sum_{1}^{N}y^i\\ln(p^i) + (1-y^i)\\ln(1-p^i),$$ where $p^i=\\sigma(\\mathscr{l}^i)=\\sigma(\\theta^T \\mathbf{x}^i + b)$. We will be optimizing this objective via gradient descent.\n",
    "\n",
    "So, to recap:\n",
    "\n",
    "- calculate the logit $\\mathscr{l}=\\theta^T \\mathbf{x} + b$,\n",
    "- calculate $\\sigma(\\mathscr{l})$, i.e. the predicted probability for a given example,\n",
    "- calculate the cross entropy loss,\n",
    "- perform gradient descent on parameters $\\theta$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "As a brief recap, *stochastic gradient descent* is an iterative method for optimizing an *objective function* (or *criterion*; when minimizing also *cost function*, *loss function*, or *error function*) which we can calculate the gradients of.\n",
    "\n",
    "One way of minimizing the cost function $L(X; \\theta)$ for a set of data $X \\in \\mathbb{R}^{NxD}$ is calculating the average cost of all the elements $\\mathbf{x} \\in X$:\n",
    "\n",
    "$$L(X; \\theta) = \\frac{1}{N} \\sum_i L(\\mathbf{x}_i; \\theta).$$\n",
    "\n",
    "Next one could calculate the gradient of this and use that to minimize the function:\n",
    "\n",
    "$$\\theta_{new}=\\theta_{old} -\\alpha \\nabla_\\theta L(X; \\theta),$$\n",
    "\n",
    "with $\\alpha$ being the step size. We would then apply this iteratively until convergence.\n",
    "\n",
    "![gradient descent](figures/fig4.png)\n",
    "<center>Source: <a href=\"https://www.deeplearningbook.org/contents/numerical.html\">Chapter 4</a> of the Deep Learning book.</center>\n",
    "\n",
    "In practice, our dataset could turn out to be enormous. It would be impractical to calculate the loss (and the gradient) for the whole dataset. We usually replace that with the cost function and gradient over a subset of $X$, a so-called *batch* $B \\subsetneq X$:\n",
    "\n",
    "$$L(B; \\theta) = \\frac{1}{|B|} \\sum_{\\mathbf{x} \\in B} L(\\mathbf{x}; \\theta).$$\n",
    "\n",
    "Doing SGD instead of GD also has good consequences for generalization, which we might talk about in the future.\n",
    "\n",
    "The gradient of the cost calculated on the batch is probably going to be different than the gradient calculated on the whole dataset, but we can use it as an approximation, trading off iteration time against convergence rate:\n",
    "\n",
    "\n",
    "$$\\nabla_\\theta L(B; \\theta) \\approx \\nabla_\\theta L(X; \\theta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial logistic regression\n",
    "\n",
    "In the multinomial case (i.e. where we want to predict a discrete variable with multiple values) we will create $K$ linear predictors instead of one, where $K$ is the number of classes. We can accomplish this by replacing the parameter vector $\\theta$ with a matrix $\\Theta \\in \\mathbb{R}^{D\\times K}$, where the $i$-th row corresponds to the $i$-th predictor, and by replacing the bias $b$ by a bias vector $\\mathbf{b}\\in R^K$. Then $\\mathscr{l}=\\mathbf{x}\\Theta^T+\\mathbf{b} \\in \\mathbb{R}^K$ (this is also called the logit in machine learning, though that's not exactly true from a mathematical point of view).\n",
    "\n",
    "The logistic function gets replaced by $\\mathtt{softmax}\\colon\\mathbb{R}^K \\rightarrow \\mathbb{R}^K$ (perhaps more precisely [*softargmax*](https://en.wikipedia.org/wiki/Softmax_function)): $$\\mathtt{softmax}(\\mathscr{l})_i = p_i = \\frac{e^{\\mathscr{l}_i}}{\\sum_{k=1}^K e^{\\mathscr{l}_k}}$$\n",
    "\n",
    "The cost function is (again) cross entropy, i.e., for a single example:\n",
    "$$-\\sum_{k=1}^K \\ln(\\mathtt{softmax}(\\mathscr{l}))_{k}\\mathbb{1}(k=y)= -\\ln(\\mathtt{softmax}(\\mathscr{l}))_{y},$$\n",
    "i.e. in the output we check the dimension corresponding to the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the data. We'll use [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) today, which is a drop-in replacement for the original [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of handwritten digits.\n",
    "![img](https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/doc/img/fashion-mnist-sprite.png) The `torchvision` package is part of the PyTorch project and includes many datasets, model architectures, and image transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are in [PIL](https://pillow.readthedocs.io/en/stable/) format, hence we need to convert them to our purposes. `torchvision` can also help with that via [transforms](https://pytorch.org/vision/0.8/transforms.html). We need to do two things:\n",
    "- convert the images to PyTorch tensors,\n",
    "- change their shape from 28x28 to 784 (a flat vector), as we'll just be using basic logistic regression and not e.g. convolutional neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Compose, Lambda\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "data = FashionMNIST(root='.', \n",
    "                    download=True, \n",
    "                    train=True, \n",
    "                    transform=Compose([ToTensor(), \n",
    "                                       Lambda(lambda x: x.view(-1))]))             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also supplies [data loaders](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), which allow us to easily batch and shuffle the data without getting bogged down in the details every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(data, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(data_loader))  # a trick to get a single batch from the dataloader\n",
    "\n",
    "plt.axis('off')\n",
    "_ = plt.imshow(image_batch[0].view(28, 28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (1p)\n",
    "\n",
    "Implement (multinomial) logistic regression in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = None \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def fit(self, data, lr=0.1, n_epochs=5):\n",
    "        self.weights = torch.randn((self.input_dim, self.output_dim), requires_grad=True)\n",
    "        self.bias = torch.randn(self.output_dim, requires_grad=True)\n",
    "        \n",
    "        data_loader = DataLoader(data, batch_size=16)\n",
    "        \n",
    "        losses, accs = [], []\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "                self.weights.requires_grad = True\n",
    "                self.bias.requires_grad = True\n",
    "                \n",
    "                predictions = self.predict_proba(images) \n",
    "                loss_val = self.cross_entropy_loss(predictions, targets)\n",
    "                loss_val.backward()  # PyTorch calculates gradients for us\n",
    "                accuracy = (self.predict(images) == targets).float().mean()\n",
    "                \n",
    "                w_grad = self.weights.grad\n",
    "                b_grad = self.bias.grad\n",
    "                with torch.no_grad():\n",
    "                    self.weights = self.weights - lr * w_grad\n",
    "                    self.bias = self.bias - lr * b_grad\n",
    "                \n",
    "                if batch_idx == 0:\n",
    "                    losses.append(loss_val.item())\n",
    "                    accs.append(accuracy.item())\n",
    "        \n",
    "        return losses, accs\n",
    "    \n",
    "    \n",
    "    def softmax(self, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        h: torch.Tensor, shape: (batch_size, output_dim)\n",
    "            Logits\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor, shape: (batch_size, output_dim)\n",
    "            Softmax on h\n",
    "        \"\"\"\n",
    "        # hint: use keepdim=True when summing\n",
    "        return ???\n",
    "        \n",
    "    def predict_proba(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: torch.Tensor, shape: (batch_size, input_dim)\n",
    "            Images\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor, shape: (batch_size, output_dim)\n",
    "            Probabilities\n",
    "        \"\"\"\n",
    "        # hint: use softmax and matrix multiplication with weights\n",
    "        return ??? \n",
    "            \n",
    "    def predict(self, X: torch.Tensor) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: torch.Tensor, shape: (batch_size, input_size)\n",
    "            Images\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor, shape: (batch_size,)\n",
    "            index of X with maximum probability\n",
    "        \"\"\"\n",
    "        # hint: use predict_proba and torch.argmax()\n",
    "        return ??? \n",
    "    \n",
    "    def cross_entropy_loss(self, y_pred: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred: torch.Tensor, shape: (batch_size, output_dim)\n",
    "            Probabilities\n",
    "        y: torch.Tensor, shape: (batch_size,)\n",
    "            Correct classes\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor, shape: ()\n",
    "            Cross entropy loss\n",
    "        \"\"\"\n",
    "        \n",
    "        # hint: use y as indexes to retrieve appropriate columns of y_pred\n",
    "        # hint: range(len(y)) in 1st dim to get all values\n",
    "        return ???     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(784, 10)\n",
    "losses, accs = log_reg.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "_ = plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "_ = plt.plot(accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
